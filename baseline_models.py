# -*- coding: utf-8 -*-
"""Baseline_Models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FqPaU36l2Nzw0H8wVyyjDSUP_oBLQ5Gg
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_squared_error as mse
import tensorflow as tf
from tensorflow import keras
import xgboost as xgb

data = pd.read_csv('https://www.dropbox.com/s/hdiyuipgmas8w3h/supercon.csv?dl=1')
critical_temp = data["critical_temp"]
features = data[["mean_atomic_mass", "mean_fie", "mean_atomic_radius", "mean_Density", "mean_ElectronAffinity", "mean_FusionHeat", "mean_ThermalConductivity", "mean_Valence" ]]
(xTrain, xTest, yTrain, yTest) = train_test_split(features, critical_temp, test_size = .4, random_state=17)

lr = LinearRegression()
lr.fit(xTrain, yTrain)
predictions = lr.predict(xTest)

plt.scatter(yTest, predictions, color = 'red')
err = mse(yTest, predictions)
plt.title('mean squared error ' + str(err))
plt.show()

model = tf.keras.Sequential()
model.add(tf.keras.Input(shape=(8,)))

#model.add(tf.keras.layers.Dense(10, activation = 'relu', kernel_initializer='glorot_normal')
#prevent overfitting - dropout - prevents model from learning features that are too specific to training dataset, pooling
model.add(tf.keras.layers.Dense(20, activation = 'selu', kernel_initializer='glorot_normal'))
#model.add(tf.keras.layers.Dropout(rate))
model.add(tf.keras.layers.Dense(14, activation = 'elu', kernel_initializer='glorot_normal'))
model.add(tf.keras.layers.Dense(10, activation = 'relu', kernel_initializer='glorot_normal'))
model.add(tf.keras.layers.Dense(1))

opt1 = keras.optimizers.Adam(learning_rate=0.01)
opt2 = keras.optimizers.SGD(learning_rate=0.1)
model.compile(optimizer=opt1, loss='mse')

model.fit(xTrain, yTrain, epochs=10)

predictions = model.predict(xTest)

reg = xgb.XGBRegressor()
#max_depth=8, min_child_weight = 0.8, eta = 0.1, gamma=0.1, subsample=1
reg.fit(xTrain, yTrain)
predictions = reg.predict(xTest)

plt.scatter(yTest, predictions, color = 'red')
#plt.ylim([-20,100])
err = mse(yTest, predictions)
plt.title('mean squared error ' + str(err))
plt.show()